// ============================================================
// DEEPGRAM WEBSOCKET LIVE STREAMING VERSION
// ============================================================
console.log('=== OFFSCREEN.JS STARTED - WEBSOCKET LIVE MODE ===');

// API ÐºÐ»ÑŽÑ‡Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽÑ‚ÑÑ Ð¸Ð· secrets.js (Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð² .gitignore)
const DEEPGRAM_KEY = window.SECRETS?.DEEPGRAM_API_KEY || '';
const OPENAI_KEY = window.SECRETS?.OPENAI_API_KEY || '';

if (!DEEPGRAM_KEY || !OPENAI_KEY) {
  console.error(
    'âŒ API keys not found! Make sure secrets.js exists and contains SECRETS object',
  );
}

// ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³Ð¸
console.log('ðŸ“‹ LANGUAGE_CONFIG loaded:', !!window.LANGUAGE_CONFIG);
console.log('ðŸŽ¤ VOICE_CONFIG loaded:', !!window.VOICE_CONFIG);

let audioStream = null,
  audioContext = null;
let analyser = null,
  gainNode = null,
  playbackContext = null;
let activeSettings = null,
  isRecording = false,
  currentTabId = null;
let speechQueue = [],
  isPlaying = false,
  history = [];

// WebSocket Ð´Ð»Ñ Deepgram
let deepgramSocket = null;
let mediaRecorder = null;
let workletNode = null;

// Ð ÐµÐ¶Ð¸Ð¼ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹: 'websocket' (Deepgram live) Ð¸Ð»Ð¸ 'whisper' (OpenAI batch)
let captureMode = 'websocket';

// --- ÐœÐ•Ð¥ÐÐÐ˜Ð—Ðœ Ð’Ð«Ð–Ð˜Ð’ÐÐÐ˜Ð¯ (KEEP-ALIVE) ---
setInterval(() => {
  if (isRecording) {
    chrome.runtime
      .sendMessage({ type: 'OFFSCREEN_KEEP_ALIVE' })
      .catch(() => {});
    // Ð¢Ð°ÐºÐ¶Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼ WebSocket Ð¶Ð¸Ð²Ñ‹Ð¼
    if (deepgramSocket && deepgramSocket.readyState === WebSocket.OPEN) {
      deepgramSocket.send(JSON.stringify({ type: 'KeepAlive' }));
    }
  }
}, 10000);

chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
  console.log('ðŸŽ§ Offscreen received:', request.type);

  if (request.type === 'PING') {
    sendResponse({ success: true });
    return true;
  }

  if (request.type === 'START_CAPTURE') {
    console.log('ðŸš€ WebSocket LIVE capture starting...');
    initCapture(request.streamId, request.settings, request.tabId)
      .then((res) => sendResponse(res))
      .catch((err) => sendResponse({ success: false, error: err.message }));
    return true;
  }

  if (request.type === 'STOP_CAPTURE') {
    stopRecording();
    sendResponse({ success: true });
  }

  if (request.type === 'UPDATE_SETTINGS') {
    console.log('âš™ï¸ Updating settings:', request.settings);
    activeSettings = { ...activeSettings, ...request.settings };
    updateVolume();
    sendResponse({ success: true });
  }

  if (request.type === 'UPDATE_VOLUME') {
    console.log('ðŸ”Š Updating volume:', request.settings);
    activeSettings = { ...activeSettings, ...request.settings };
    updateVolume();
    sendResponse({ success: true });
  }

  if (request.type === 'UPDATE_VOICE') {
    console.log('ðŸŽ¤ Updating voice:', request.settings);
    activeSettings = { ...activeSettings, ...request.settings };
    sendResponse({ success: true });
  }

  if (request.type === 'UPDATE_SETTINGS_FROM_POPUP') {
    console.log('ðŸ“¨ Updating settings from popup:', request.settings);
    activeSettings = { ...activeSettings, ...request.settings };
    updateVolume();
    sendResponse({ success: true });
  }

  return true;
});

// ============================================================
// WEBSOCKET CONNECTION TO DEEPGRAM
// ============================================================
function connectDeepgramWebSocket(lang) {
  return new Promise((resolve, reject) => {
    const model = 'nova-3';

    // URL Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸ Ð´Ð»Ñ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸
    const wsUrl =
      `wss://api.deepgram.com/v1/listen?` +
      `model=${model}` +
      `&language=${lang}` +
      `&encoding=linear16` + // PCM 16-bit
      `&sample_rate=16000` + // 16kHz
      `&channels=1` + // ÐœÐ¾Ð½Ð¾
      `&interim_results=true` +
      `&endpointing=300` + // ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ñ†Ð° Ñ„Ñ€Ð°Ð·Ñ‹ Ñ‡ÐµÑ€ÐµÐ· 300ms Ñ‚Ð¸ÑˆÐ¸Ð½Ñ‹
      `&punctuate=true` +
      `&smart_format=true`;

    console.log('ðŸ”Œ Connecting to Deepgram WebSocket:', wsUrl);

    deepgramSocket = new WebSocket(wsUrl, ['token', DEEPGRAM_KEY]);

    deepgramSocket.onopen = () => {
      console.log('âœ… Deepgram WebSocket CONNECTED!');
      resolve();
    };

    deepgramSocket.onmessage = (event) => {
      try {
        const data = JSON.parse(event.data);

        // ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ
        const transcript = data.channel?.alternatives?.[0]?.transcript;
        const isFinal = data.is_final;
        const confidence = data.channel?.alternatives?.[0]?.confidence || 0;

        console.log(
          `ðŸ“ Deepgram: "${transcript}" (final: ${isFinal}, conf: ${confidence})`,
        );

        // ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼
        if (transcript && transcript.trim().length > 2 && isFinal) {
          console.log('ðŸŽ¯ Final transcript, sending to translate:', transcript);
          translateAndVoice(transcript);
        }
      } catch (e) {
        console.error('âŒ Error parsing Deepgram response:', e);
      }
    };

    deepgramSocket.onerror = (error) => {
      console.error('âŒ Deepgram WebSocket ERROR:', error);
      reject(error);
    };

    deepgramSocket.onclose = (event) => {
      console.log('ðŸ“´ Deepgram WebSocket closed:', event.code, event.reason);
      // ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¿ÐµÑ€ÐµÐ¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ ÐµÑÐ»Ð¸ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð°
      if (isRecording && event.code !== 1000) {
        console.log('ðŸ”„ Attempting to reconnect...');
        setTimeout(() => {
          if (isRecording) {
            connectDeepgramWebSocket(
              activeSettings?.sourceLanguage || 'en',
            ).catch(console.error);
          }
        }, 2000);
      }
    };
  });
}

// ============================================================
// MAIN CAPTURE FUNCTION (HYBRID: WebSocket for nova-3, Whisper for he/ar/fa)
// ============================================================
async function initCapture(streamId, settings, tabId) {
  try {
    if (isRecording) stopRecording();

    activeSettings = settings;
    currentTabId = tabId;
    const lang = settings?.sourceLanguage || 'en';

    // ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ€ÐµÐ¶Ð¸Ð¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³Ð° ÑÐ·Ñ‹ÐºÐ°
    const langConfig = (window.LANGUAGE_CONFIG &&
      window.LANGUAGE_CONFIG[lang]) ||
      window.LANGUAGE_CONFIG?.default || { model: 'nova-3' };

    // Ð•ÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ whisper-* â€” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ batch mode (OpenAI Whisper)
    captureMode = langConfig.model?.startsWith('whisper')
      ? 'whisper'
      : 'websocket';

    console.log(
      `ðŸŽ¤ Starting capture: lang=${lang}, mode=${captureMode}, model=${langConfig.model}`,
    );

    // 1. Ð—Ð°Ñ…Ð²Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð°ÑƒÐ´Ð¸Ð¾ Ñ Ð²ÐºÐ»Ð°Ð´ÐºÐ¸
    audioStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        mandatory: {
          chromeMediaSource: 'tab',
          chromeMediaSourceId: streamId,
        },
      },
    });
    console.log('âœ… Audio stream obtained');

    // 2. ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ AudioContext Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ñ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚Ð¸
    // Ð”Ð»Ñ WebSocket Ð½ÑƒÐ¶ÐµÐ½ 16kHz, Ð´Ð»Ñ Whisper Ð¼Ð¾Ð¶Ð½Ð¾ 48kHz (Ð½Ð¾ 16kHz Ñ‚Ð¾Ð¶Ðµ Ð¾Ðº)
    const sampleRate = captureMode === 'websocket' ? 16000 : 48000;
    audioContext = new AudioContext({ sampleRate });
    const source = audioContext.createMediaStreamSource(audioStream);
    analyser = audioContext.createAnalyser();
    gainNode = audioContext.createGain();

    source.connect(analyser);
    source.connect(gainNode);
    gainNode.connect(audioContext.destination);

    isRecording = true;
    updateVolume();

    if (captureMode === 'websocket') {
      // ============ WEBSOCKET MODE (Deepgram Live) ============
      await connectDeepgramWebSocket(lang);
      console.log('âœ… Deepgram WebSocket connected');

      // Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ AudioWorklet Ð´Ð»Ñ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ð¸ Ð² PCM
      await audioContext.audioWorklet.addModule('pcm-processor.js');
      console.log('âœ… PCM Processor loaded');

      workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');

      // ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ PCM Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‚ worklet Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð² WebSocket
      workletNode.port.onmessage = (event) => {
        if (
          !isRecording ||
          !deepgramSocket ||
          deepgramSocket.readyState !== WebSocket.OPEN
        ) {
          return;
        }
        deepgramSocket.send(event.data);
      };

      source.connect(workletNode);
      workletNode.connect(audioContext.destination);

      console.log('âœ… WebSocket pipeline ready');
    } else {
      // ============ WHISPER MODE (OpenAI Batch) ============
      console.log('ðŸŽ™ï¸ Using Whisper mode for', lang);
      startWhisperLoop(langConfig.interval || 5000);
    }

    console.log('âœ… Capture started for tab:', tabId);
    return { success: true };
  } catch (e) {
    console.error('âŒ Init Error:', e);
    isRecording = false;
    return { success: false, error: e.message };
  }
}

// ============================================================
// WHISPER MODE (Batch processing for Hebrew, Arabic, Persian)
// ============================================================
let lastWhisperText = '';

function startWhisperLoop(interval) {
  if (!isRecording || !audioStream) return;

  console.log(`ðŸŽ™ï¸ Starting Whisper loop with ${interval}ms interval`);

  mediaRecorder = new MediaRecorder(audioStream, {
    mimeType: 'audio/webm;codecs=opus',
  });
  let chunks = [];

  mediaRecorder.ondataavailable = (e) => {
    if (e.data.size > 0) chunks.push(e.data);
  };

  mediaRecorder.onstop = async () => {
    if (chunks.length > 0 && isRecording) {
      const blob = new Blob(chunks, { type: 'audio/webm' });

      // ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð·Ð²ÑƒÐºÐ°
      const data = new Uint8Array(analyser.frequencyBinCount);
      analyser.getByteFrequencyData(data);
      const avgVolume = data.reduce((a, b) => a + b) / data.length;

      if (avgVolume > 2) {
        await processWhisperSTT(blob);
      }
    }
    // ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ñ†Ð¸ÐºÐ»
    if (isRecording) {
      setTimeout(() => startWhisperLoop(interval), 50);
    }
  };

  mediaRecorder.start();
  setTimeout(() => {
    if (mediaRecorder?.state === 'recording') mediaRecorder.stop();
  }, interval);
}

async function processWhisperSTT(blob) {
  if (!isRecording) return;

  try {
    const lang = activeSettings?.sourceLanguage || 'en';
    console.log(`ðŸŽ™ï¸ Sending to Whisper (${lang})...`);

    const fd = new FormData();
    fd.append('file', blob, 'audio.webm');
    fd.append('model', 'whisper-1');
    fd.append('language', lang);
    fd.append('response_format', 'json');

    const res = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: { Authorization: `Bearer ${OPENAI_KEY}` },
      body: fd,
    });

    const data = await res.json();
    const text = data?.text?.trim();

    if (text && text.length > 2 && text !== lastWhisperText) {
      lastWhisperText = text;
      console.log('ðŸŽ™ï¸ Whisper transcript:', text);
      translateAndVoice(text);
    }
  } catch (e) {
    console.error('âŒ Whisper STT Error:', e);
  }
}

// ============================================================
// TRANSLATION
// ============================================================
async function translateAndVoice(text) {
  if (!isRecording || !text?.trim()) return;

  try {
    const targetLang = activeSettings?.targetLanguage || 'ru';
    const style = (activeSettings?.translationStyle || 'DEFAULT').toUpperCase();

    console.log(`ðŸŒ Translating to ${targetLang}:`, text);

    const prompts = {
      // DEFAULT: `Professional interpreter. Translate to ${targetLang}. Natural spoken style. ONLY translation.`,
      // KABBALAH: `Translate to ${targetLang}. Use Kabbalah terms (Light, Vessel, Screen). ONLY translation.`,
      // KIDS: `Translate to ${targetLang} as a fairy tale. Simple words. ONLY translation.`
      DEFAULT: `Translate to ${targetLang} accurately and naturally. Keep original meaning.`,
      KIDS: `Translate to ${targetLang} for a 5-year-old child. Use simple words, fairy tale style. Make it magical and fun!`,
      KABBALAH: `Translate to ${targetLang} using Kabbalah concepts.  Use Kabbalah terms (Light, Vessel, Screen). ONLY translation.`,
      TECHNICAL: `Technical translation to ${targetLang}. Keep all terms exact (do NOT simplify). Use formal style.`,
      SLANG: `Translate to ${targetLang} using modern youth slang, memes, casual speech. Sound like a TikTok teen.`,
      POETIC: `Translate to ${targetLang} poetically. Use metaphors, beautiful language, rhythm. Make it sound like a poem.`,
    };

    const res = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${OPENAI_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: prompts[style] || prompts.DEFAULT },
          ...history.slice(-6),
          { role: 'user', content: text },
        ],
        temperature: 0.1,
      }),
    });

    const data = await res.json();
    const translatedText = data?.choices?.[0]?.message?.content;

    if (!translatedText) {
      console.error('âŒ No translation received');
      return;
    }

    console.log('âœ… Translation:', translatedText);

    // ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° ÑÑƒÐ±Ñ‚Ð¸Ñ‚Ñ€Ð¾Ð²
    chrome.runtime
      .sendMessage({
        type: 'SUBTITLES_FROM_OFFSCREEN',
        text: translatedText,
        tabId: currentTabId,
      })
      .catch(() => {});

    history.push(
      { role: 'user', content: text },
      { role: 'assistant', content: translatedText },
    );
    if (history.length > 12) history.splice(0, 2);

    if (activeSettings?.enableVoice) playTTS(translatedText);
  } catch (e) {
    console.error('âŒ Translation Error:', e);
  }
}

// ============================================================
// TTS
// ============================================================
async function playTTS(text) {
  if (!isRecording || !text?.trim()) return;

  try {
    const voiceKey = activeSettings?.voiceGender || 'neutral';
    const voice =
      (window.VOICE_CONFIG && window.VOICE_CONFIG[voiceKey]) || 'nova';
    console.log(`ðŸŽ­ TTS with voice: ${voiceKey} -> ${voice}`);

    const res = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${OPENAI_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ model: 'tts-1', voice, input: text, speed: 1.05 }),
    });

    const buffer = await res.arrayBuffer();
    if (!playbackContext) playbackContext = new AudioContext();

    const audioBuffer = await playbackContext.decodeAudioData(buffer);
    speechQueue.push(audioBuffer);
    if (!isPlaying) handleQueue();
  } catch (e) {
    console.error('TTS Error:', e);
  }
}

function handleQueue() {
  if (!speechQueue.length || !isRecording) {
    isPlaying = false;
    return;
  }

  isPlaying = true;
  const source = playbackContext.createBufferSource();
  source.buffer = speechQueue.shift();
  source.connect(playbackContext.destination);
  source.onended = handleQueue;
  source.start(0);

  if (playbackContext.state === 'suspended') playbackContext.resume();
}

// ============================================================
// VOLUME CONTROL
// ============================================================
function updateVolume() {
  if (!gainNode || !audioContext) {
    console.log('ðŸ”‡ Volume update skipped - no audio context');
    return;
  }
  const vol = activeSettings?.muteOriginal
    ? 0
    : activeSettings?.originalVolume || 1;
  console.log(
    'ðŸ”Š Setting volume to:',
    vol,
    'mute:',
    activeSettings?.muteOriginal,
  );
  gainNode.gain.setTargetAtTime(vol, audioContext.currentTime, 0.1);
}

// ============================================================
// STOP
// ============================================================
function stopRecording() {
  console.log('ðŸ›‘ Stopping recording...');
  isRecording = false;
  isPlaying = false;
  speechQueue = [];
  history = [];
  lastWhisperText = '';

  // Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ WebSocket (ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»)
  if (deepgramSocket) {
    deepgramSocket.close(1000, 'User stopped');
    deepgramSocket = null;
  }

  // ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ worklet (ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»)
  if (workletNode) {
    workletNode.disconnect();
    workletNode = null;
  }

  if (mediaRecorder?.state !== 'inactive') mediaRecorder?.stop();
  mediaRecorder = null;

  if (audioStream) audioStream.getTracks().forEach((t) => t.stop());
  if (audioContext) audioContext.close().catch(() => {});
  if (playbackContext) playbackContext.close().catch(() => {});

  audioContext = null;
  playbackContext = null;
  audioStream = null;
  captureMode = 'websocket';

  console.log('âœ… Recording stopped');
}
